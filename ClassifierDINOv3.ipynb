{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/obete/ClassifierDINOv3/blob/main/ClassifierDINOv3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz7rYU9Jqx4P"
      },
      "source": [
        "## **DINOv3 as base with a classifier head**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PdkzefKOiObU"
      },
      "outputs": [],
      "source": [
        "#@title Dependencies and Modules\n",
        "!pip install tensorflow pillow requests\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPiS5_xlFtYA"
      },
      "source": [
        "**Dataset Used**\n",
        "\n",
        "The data used in this project is Fitzpartick17k dataset available in kaggle. Its a csv comprising of approximately 17,000 record of image url, fitzpatick scale and labels of skin condition for each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jq3lHk_qjNmb"
      },
      "outputs": [],
      "source": [
        "#@title Setting Data Path\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file_path = \"/content/drive/My Drive/Colab Notebooks/AI_ML_EXAM/fitzpatrick17k.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "q9huCz2TGk-D"
      },
      "outputs": [],
      "source": [
        "#@title Loading Dataset Google drive onto this notebook\n",
        "skin_data = pd.read_csv(file_path)\n",
        "skin_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwoVmPxCHIYa"
      },
      "source": [
        "**FIltering the Data**\n",
        "\n",
        "This study is focusing on the Afican skin which falls under fitzpatrick scal 4, 5 and 6. There fore not all the 17,000 images in the data set will be useful for this project. We filtered out to remain with only desired scale.\n",
        "This leave us with 4934 images corresponding to the desired skin color."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "K25ncW6bisX_"
      },
      "outputs": [],
      "source": [
        "#@title Filtering Data for Fitzpatrick Scale 5 and 6 and dropping the empty urls\n",
        "filtered_data = skin_data[skin_data['fitzpatrick_scale'].isin([5,6]) & skin_data['url'].notna()]\n",
        "filtered_data[['url','label','fitzpatrick_scale']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bt0HTv_VkEF6"
      },
      "outputs": [],
      "source": [
        "# Target folder under My Drive\n",
        "DRIVE_ROOT = '/content/drive/MyDrive/Colab Notebooks/AI_ML_EXAM/'\n",
        "base_dir = f'{DRIVE_ROOT}/skin_images'\n",
        "os.makedirs(base_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oddBmqNqIRaD"
      },
      "source": [
        "**Accessing Images.**\n",
        "\n",
        "The .csv fil provides url to the public location of the images. Therefore, to make use of it, in this section, we download, resize  and save the images in their correct labels in our google drive in a folder skin_images.                                                                   This section does that and takes a count of the total number of images that have successfully been downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahDbBUuPkUHW"
      },
      "outputs": [],
      "source": [
        "#@title Downloading and Resizing Images\n",
        "from pathlib import Path\n",
        "# Target folder under My Drive\n",
        "DRIVE_ROOT = '/content/drive/MyDrive/Colab Notebooks/AI_ML_EXAM/'\n",
        "base_dir = f'{DRIVE_ROOT}/skin_images'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Remove duplicates from the dataset\n",
        "filtered_data = filtered_data.drop_duplicates(subset=['url'])\n",
        "\n",
        "# Headers to mimic a browser request\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# Initialize counters\n",
        "total_images_downloaded = 0\n",
        "failed_urls = []\n",
        "\n",
        "# Download, resize, and save images\n",
        "for idx, row in filtered_data.iterrows():\n",
        "    lbl, url = row['label'], row['url']\n",
        "    outdir = os.path.join(base_dir, lbl)\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    try:\n",
        "        # Use headers in the request\n",
        "        resp = requests.get(url, headers=headers, timeout=10)  # Add headers\n",
        "        resp.raise_for_status()\n",
        "\n",
        "        # Process and save the image\n",
        "        img = Image.open(BytesIO(resp.content)).convert('RGB')\n",
        "        img = img.resize((224, 224))\n",
        "\n",
        "        # Ensure unique filenames\n",
        "        unique_filename = f\"{idx}_{hash(url)}.jpg\"\n",
        "        img.save(os.path.join(outdir, unique_filename))\n",
        "        total_images_downloaded += 1\n",
        "    except Exception as e:\n",
        "        failed_urls.append((url, str(e)))  # Log failed URLs\n",
        "\n",
        "# Verify and count total files\n",
        "file_count = sum(1 for _ in Path(base_dir).rglob('*.jpg'))\n",
        "\n",
        "# Display results\n",
        "print(f\"Total images downloaded and stored: {total_images_downloaded}\")\n",
        "print(f\"Total image files found in folder and subfolders: {file_count}\")\n",
        "print(f\"Failed URLs: {len(failed_urls)}\")\n",
        "\n",
        "# Optional: Save failed URLs for review\n",
        "failed_urls_path = f'{DRIVE_ROOT}/failed_urls.txt'\n",
        "with open(failed_urls_path, 'w') as f:\n",
        "    for url, error in failed_urls:\n",
        "        f.write(f\"{url}\\t{error}\\n\")\n",
        "print(f\"Failed URLs logged to: {failed_urls_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import shutil\n",
        "\n",
        "for class_name in os.listdir(f'{DRIVE_ROOT}/skin_images'):\n",
        "    class_path = os.path.join(f'{DRIVE_ROOT}/skin_images', class_name)\n",
        "    if os.path.isdir(class_path):\n",
        "        train_class_path = f\"{DRIVE_ROOT}/train_set/{class_name}\"\n",
        "        val_class_path = f\"{DRIVE_ROOT}/val_set/{class_name}\"\n",
        "        os.makedirs(train_class_path, exist_ok=True)\n",
        "        os.makedirs(val_class_path, exist_ok=True)\n",
        "\n",
        "        # List all images\n",
        "        images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
        "        random.shuffle(images)\n",
        "\n",
        "        # Split 80/20\n",
        "        split_idx = int(0.8 * len(images))\n",
        "        train_files = images[:split_idx]\n",
        "        val_files = images[split_idx:]\n",
        "\n",
        "        # Copy into train/\n",
        "        for f in train_files:\n",
        "            shutil.copy(os.path.join(class_path, f), os.path.join(train_class_path, f))\n",
        "\n",
        "        # Copy into val/\n",
        "        for f in val_files:\n",
        "            shutil.copy(os.path.join(class_path, f), os.path.join(val_class_path, f))"
      ],
      "metadata": {
        "id": "KMt5rYzhRVjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# For ViT-Small/Base: 224x224, for ViT-Large: 384x384\n",
        "img_size = 224\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                         std=(0.229, 0.224, 0.225))\n",
        "])\n"
      ],
      "metadata": {
        "id": "8zmHz9VzOrhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dir = f'{DRIVE_ROOT}/train_set'\n",
        "val_dir = f'{DRIVE_ROOT}/val_set'\n",
        "\n",
        "train_dataset = ImageFolder(root=train_dir, transform=transform)\n",
        "val_dataset = ImageFolder(root=val_dir, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Class names\n",
        "print(train_dataset.classes)\n"
      ],
      "metadata": {
        "id": "VIGtKbJqOtYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Using DINOv3\n",
        "\n",
        "!pip install -U transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9oXLUIDJDToN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Local Inference on GPU\n",
        "Model page: https://huggingface.co/facebook/dinov3-vith16plus-pretrain-lvd1689m\n",
        "\n",
        "⚠️ If the generated code snippets do not work, please open an issue on either the model repo and/or on huggingface.js 🙏\n",
        "\n",
        "The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. 🤗"
      ],
      "metadata": {
        "id": "0OvFae9uDl01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ],
      "metadata": {
        "id": "jRQ8bOFXDiFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load model directly\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov3-vith16plus-pretrain-lvd1689m\")\n",
        "backbone = AutoModel.from_pretrained(\"facebook/dinov3-vith16plus-pretrain-lvd1689m\")"
      ],
      "metadata": {
        "id": "JD9GtjqDEuyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Freeze backbone → use DINOv3 as a fixed feature extractor.\n",
        "for param in backbone.parameters():\n",
        "    param.requires_grad = False  # freeze backbone\n"
      ],
      "metadata": {
        "id": "9vXUcVa_IU7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Custom Classification head\n",
        "import torch.nn as nn\n",
        "\n",
        "class DinoClassifier(nn.Module):\n",
        "    def __init__(self, model, num_classes):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.head = nn.Linear(model.config.hidden_size, num_classes)  # simple linear head\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.model(x).last_hidden_state[:,0]  # CLS token\n",
        "        return self.head(features)\n"
      ],
      "metadata": {
        "id": "z4TZvqTPIkyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "62b1a78a"
      },
      "source": [
        "!pip install torchmetrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = DinoClassifier(backbone, num_classes=len(train_dataset.classes)).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(5):  # Example: 5 epochs\n",
        "    model.train()\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "1U9Lio74W_aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Define metric objects\n",
        "train_acc_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=len(train_dataset.classes)).to(device)\n",
        "val_acc_metric   = torchmetrics.Accuracy(task=\"multiclass\", num_classes=len(train_dataset.classes)).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # ---- Training ----\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_acc_metric.reset()\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * imgs.size(0)\n",
        "        train_acc_metric.update(outputs, labels)\n",
        "\n",
        "    train_loss /= len(train_dataset)\n",
        "    train_acc = train_acc_metric.compute()\n",
        "\n",
        "    # ---- Validation ----\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_acc_metric.reset()\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * imgs.size(0)\n",
        "            val_acc_metric.update(outputs, labels)\n",
        "\n",
        "    val_loss /= len(val_dataset)\n",
        "    val_acc = val_acc_metric.compute()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} \"\n",
        "          f\"| Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} \"\n",
        "          f\"| Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QuhhvOGXb5JB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}