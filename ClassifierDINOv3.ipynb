{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/obete/ClassifierDINOv3/blob/main/ClassifierDINOv3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz7rYU9Jqx4P"
      },
      "source": [
        "## **DINOv3 as base with a classifier head**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs3FFWTnNAd-"
      },
      "outputs": [],
      "source": [
        "# Target folder under My Drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_ROOT = '/content/drive/MyDrive/Colab Notebooks/DINOv3_Classifier/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zmHz9VzOrhE"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# For ViT-Small/Base: 224x224, for ViT-Large: 384x384\n",
        "img_size = 224\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(img_size),      # random crops\n",
        "    transforms.RandomHorizontalFlip(),           # flip left-right\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),  # color variation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                         std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),     # no randomness\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                         std=(0.229, 0.224, 0.225))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIGtKbJqOtYL"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dir = f'{DRIVE_ROOT}/train_set'\n",
        "val_dir = f'{DRIVE_ROOT}/val_set'\n",
        "\n",
        "train_dataset = ImageFolder(root=train_dir, transform=train_transform)\n",
        "val_dataset = ImageFolder(root=val_dir, transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Class names\n",
        "print(train_dataset.classes)\n",
        "num_classes = len(train_dataset.classes)\n",
        "print(num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9oXLUIDJDToN"
      },
      "outputs": [],
      "source": [
        "#@title Using DINOv3\n",
        "\n",
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OvFae9uDl01"
      },
      "source": [
        "Local Inference on GPU\n",
        "Model page: https://huggingface.co/facebook/dinov3-vith16plus-pretrain-lvd1689m\n",
        "\n",
        "âš ï¸ If the generated code snippets do not work, please open an issue on either the model repo and/or on huggingface.js ðŸ™\n",
        "\n",
        "The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. ðŸ¤—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRQ8bOFXDiFh"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JD9GtjqDEuyW"
      },
      "outputs": [],
      "source": [
        "# @title Load model directly\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov3-vith16plus-pretrain-lvd1689m\")\n",
        "backbone = AutoModel.from_pretrained(\"facebook/dinov3-vith16plus-pretrain-lvd1689m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vXUcVa_IU7M"
      },
      "outputs": [],
      "source": [
        "#@title Freeze backbone â†’ use DINOv3 as a fixed feature extractor.\n",
        "for param in backbone.parameters():\n",
        "    param.requires_grad = False  # freeze backbone\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4TZvqTPIkyd"
      },
      "outputs": [],
      "source": [
        "#@title Custom Classification head\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class DinoClassifier(nn.Module):\n",
        "    def __init__(self, backbone, num_classes):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        hidden_size = backbone.config.hidden_size\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x).last_hidden_state[:, 0]  # CLS token\n",
        "        return self.head(features)\n",
        "\n",
        "model = DinoClassifier(backbone, num_classes).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "62b1a78a"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "QuhhvOGXb5JB"
      },
      "outputs": [],
      "source": [
        "import torchmetrics\n",
        "\n",
        "num_epochs = 150\n",
        "lr = 1e-4\n",
        "best_val_acc = 0.0\n",
        "\n",
        " #Loss, optimizer, metrics\n",
        "# -------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)  # decay LR\n",
        "\n",
        "train_acc_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
        "val_acc_metric   = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
        "\n",
        "\n",
        "for epoch in range(150):\n",
        "    # ---- Training ----\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_acc_metric.reset()\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * imgs.size(0)\n",
        "        train_acc_metric.update(outputs, labels)\n",
        "\n",
        "    train_loss /= len(train_dataset)\n",
        "    train_acc = train_acc_metric.compute()\n",
        "\n",
        "    # ---- Validation ----\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_acc_metric.reset()\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * imgs.size(0)\n",
        "            val_acc_metric.update(outputs, labels)\n",
        "\n",
        "    val_loss /= len(val_dataset)\n",
        "    val_acc = val_acc_metric.compute()\n",
        "\n",
        "    # ---- Save best checkpoint ----\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_dinov3_classifier.pth\")\n",
        "\n",
        "    # ---- Scheduler step ----\n",
        "    scheduler.step()\n",
        "\n",
        "    # ---- Log progress ----\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} \"\n",
        "          f\"| Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} \"\n",
        "          f\"| Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f} \"\n",
        "          f\"| LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "print(\"Training complete. Best Val Acc:\", best_val_acc.item())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}